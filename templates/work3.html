<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta content="IE=edge" http-equiv="X-UA-Compatible">
  <meta content="width=device-width,initial-scale=1" name="viewport">
  <meta content="description" name="description">
  <meta name="google" content="notranslate" />
  <meta content="Mashup templates have been developped by Orson.io team" name="author">

  <!-- Disable tap highlight on IE -->
  <meta name="msapplication-tap-highlight" content="no">
  
  <link rel="apple-touch-icon" sizes="180x180" href="./static/assets/apple-icon-180x180.png">
  <link href="./static/assets/favicon.ico" rel="icon">

  <title>M.Vidal Work</title>  

<link href="./static/main.3f6952e4.css" rel="stylesheet"></head>

<body class="">
<div id="site-border-left"></div>
<div id="site-border-right"></div>
<div id="site-border-top"></div>
<div id="site-border-bottom"></div>
<!-- Add your content of header -->
<header>
  <nav class="navbar  navbar-fixed-top navbar-default">
    <div class="container">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse" aria-expanded="false">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

      <div class="collapse navbar-collapse" id="navbar-collapse">
        <ul class="nav navbar-nav ">
          <li><a href="./index.html" title="">01 : Home</a></li>
          <li><a href="./projects.html" title="">02 : Projects</a></li>
          <li><a href="./experience.html" title="">03 : Experience</a></li>
          <li><a href="./education.html" title="">04 : Education</a></li>
          <li><a href="./contact.html" title="">05 : Contact</a></li>
        </ul>


          <ul class="nav navbar-nav navbar-right navbar-small visible-md visible-lg">
            <li><a href="./work.html" title="" class="active">001</a></li>
            <li><a href="./work2.html" title="">002</a></li>
            <li><a href="./work3.html" title="">003</a></li>
            <li><a href="./work4.html" title="">004</a></li>
            <li><a href="./work5.html" title="">005</a></li>
            <li><a href="./work6.html" title="">006</a></li>
            <li><a href="./work7.html" title="">007</a></li>
            <li><a href="./work8.html" title="">008</a></li>
            <li><a href="./work9.html" title="">009</a></li>
            <li><a href="./work10.html" title="">010</a></li>
            <li><a href="./work11.html" title="">011</a></li>
          </ul>


      </div> 
    </div>
  </nav>
</header>
<div class="section-container">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <img src="./static/assets/images/work001-01.jpg" class="img-responsive" alt="">
        <div class="card-container">
          <div class="text-center">
            <h1 class="h2">003 : Advantages of Using GridSearch or RandomSearch in Your Prediction Algorithm</h1>
          </div>
          <p> As discussed in previous articles, a data scientist’s primary goal is to achieve high accuracy in predictive models while minimizing false positives and false negatives in the confusion matrix. The challenge lies in selecting the algorithm that best fits the problem at hand. In this article, we’ll explore how to enhance accuracy by fine-tuning the hyperparameters of your chosen algorithms.<br><br>

          All machine learning algorithms are parameterized, meaning you can optimize your predictive model by adjusting these parameters. There are several techniques for hyperparameter optimization, including GridSearch, RandomSearch, Bayesian Optimization with Gaussian Processes, Hyperopt, and Optuna. However, we’ll focus on the two most commonly used methods in the industry: GridSearch and RandomSearch, both available in the Sklearn library.<br><br>

          1) sklearn.model_selection.GridSearchCV<br><br>

          GridSearchCV is a widely recognized and utilized tool. It methodically explores all possible combinations of the specified hyperparameters. However, it is important to note that GridSearchCV can become inefficient with a large number of parameters, as it evaluates every possible combination, which can lead to high computational costs and potential system overload. Despite this, it remains a powerful tool for finding the optimal parameter set when the parameter space is manageable.<br><br>
          </p>

          <div class="">
            <div class="">
              <img src="./static/assets/images/grids.jpg" class="img-responsive" alt="">
            </div>
          </div><br>

          

          <p>In the previous example, we used GridSearch to optimize the hyperparameters of the Logistic Regression algorithm from the Scikit-learn library. Logistic Regression accepts several hyperparameters, but in this case, we focused on tuning the penalty and C parameters. The test hyperparameters were stored in a variable called values_grid. To understand each hyperparameter’s role, you can refer to the Scikit-learn documentation here: [Logistic Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).<br><br>

          After training and evaluating the Logistic Regression model with different hyperparameter combinations, we found that the best performance was achieved with the parameters C = 0.001 and penalty = ‘none’, resulting in an accuracy of 95.93%.<br><br>

          I. Exhaustive Search: GridSearch performs an exhaustive search over the specified parameter grid. It evaluates every possible combination of parameters, which ensures that you find the optimal settings, but this can be computationally expensive for large parameter grids.<br><br>

          II. Pros and Cons:<br>
          — Pros: Guarantees finding the best parameter combination within the specified grid, easy to understand and implement.<br>
          — Cons: Can be very slow and resource-intensive, especially with a large number of parameters or when using a complex model.<br><br>

          III. Best Practices:<br>
          — Use with Small to Medium Grids: GridSearch works best with a manageable number of parameters and values. If the grid becomes too large, consider using RandomSearch or other optimization methods.<br>
          — Combine with Cross-Validation: Incorporate cross-validation to get a robust estimate of model performance for each parameter combination. <br><br>

          2) sklearn.model_selection.RandomizedSearchCV<br><br>


          RandomizedSearchCV is another effective method for hyperparameter optimization. Unlike GridSearch, RandomizedSearch randomly selects parameter combinations to test, making it more efficient when dealing with a large number of parameters. This approach allows for the exploration of a broader range of hyperparameters without overwhelming the machine.<br><br>

          In this example, we’ll use RandomizedSearchCV with the Decision Tree Classification algorithm (DecisionTreeClassifier) from the Scikit-learn library. Instead of evaluating every possible combination of hyperparameters, RandomizedSearchCV performs a fixed number of iterations, specified by the n_iter parameter.<br><br>

          If we were to use GridSearch for this example, it would test all values in the values_grid variable, leading to a total of 30 iterations to identify the best parameters. In contrast, RandomizedSearch allows us to define the number of iterations, which can be adjusted to balance the trade-off between computational cost and search thoroughness.<br><br>

          </p>

          <div class="">
            <div class="">
              <img src="./static/assets/images/random.jpg" class="img-responsive" alt="">
            </div>
          </div><br>

        

          <p>In our example with RandomizedSearchCV, we set 14 iterations and discovered that the optimal hyperparameters for the Decision Tree Classification algorithm were max_depth = 4 and max_features = 5, resulting in an accuracy of 95.90%.<br><br>

          I. Random Sampling: RandomSearch samples a fixed number of parameter combinations from the parameter space. This approach can be more efficient than GridSearch, especially when dealing with a high-dimensional parameter space.<br><br>

          II. Pros and Cons:<br>
          — Pros: Generally faster than GridSearch, especially for high-dimensional spaces, and can often find good parameter settings with fewer iterations.<br>
          — Cons: May miss the optimal parameter combination if the number of iterations is too low.<br><br>

          III. Best Practices:<br>
          — Adjust the Number of Iterations: The number of iterations (or n_iter) should be chosen based on available computational resources and the size of the parameter space. More iterations can improve the chances of finding optimal parameters.<br>
          — Combine with Cross-Validation: Like GridSearch, use cross-validation to evaluate each set of parameters.<br><br>

          Conclusion  <br><br>

          While there are various methods for optimizing machine learning algorithms, GridSearch and RandomSearch are among the most commonly used in the industry. Hyperparameter optimization is crucial for ensuring that your model performs optimally, balancing accuracy with computational efficiency. For data scientists, mastering these techniques is essential for fine-tuning algorithms and achieving the best possible results.<br><br>
        </p>
        </div>
      </div>

          

          <!-- <blockquote>
            <p>"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante."</p>
            <small class="pull-right">Irina Martin</small>
          </blockquote> -->
        </div>
       
     <!--  <div class="col-xs-12">
        <img src="./static/assets/images/work001-04.jpg" class="img-responsive" alt="">
      </div> -->

    </div>
  </div>
</div>


<footer class="footer-container text-center">
  <div class="container">
    <div class="row">
      <div class="col-xs-12">
        <p>© MVJR | Template download from Mashup and modified by Moacir Vidal</a></p>
      </div>
    </div>
  </div>
</footer>

<script>
  document.addEventListener("DOMContentLoaded", function (event) {
     navActivePage();
  });
</script>

<!-- Google Analytics: change UA-XXXXX-X to be your site's ID 

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-XXXXX-X', 'auto');
  ga('send', 'pageview');
</script>

--> <script type="text/javascript" src="./static/main.70a66962.js"></script></body>

</html>